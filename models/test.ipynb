{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import plotly.graph_objects as go\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "def load_model():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n",
    "    pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)\n",
    "    return pipe\n",
    "\n",
    "def estimate_depth(model, img):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        img_pil = img\n",
    "\n",
    "    predictions = model(img_pil)\n",
    "    depth_map = predictions[\"depth\"]\n",
    "    depth_map_np = np.array(depth_map).squeeze()\n",
    "    depth_map_resized = cv2.resize(depth_map_np, (img_pil.size[0], img_pil.size[1]))\n",
    "    \n",
    "    return depth_map_resized\n",
    "\n",
    "def process_image(model, image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "\n",
    "    depth_map = estimate_depth(model, image)\n",
    "    return image, depth_map\n",
    "\n",
    "def combine_depth_maps(depth_maps):\n",
    "    # Simple averaging of depth maps\n",
    "    return np.mean(depth_maps, axis=0)\n",
    "\n",
    "def create_point_cloud(images, depth_maps, sample_rate=10):\n",
    "    points = []\n",
    "    colors = []\n",
    "    \n",
    "    for img, depth in zip(images, depth_maps):\n",
    "        h, w = depth.shape\n",
    "        for y in range(0, h, sample_rate):\n",
    "            for x in range(0, w, sample_rate):\n",
    "                z = depth[y, x]\n",
    "                if z > 0:\n",
    "                    points.append([x, y, z])\n",
    "                    colors.append(img[y, x])\n",
    "    \n",
    "    return np.array(points), np.array(colors)\n",
    "\n",
    "def plot_3d_point_cloud(points, colors):\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=points[:, 0],\n",
    "        y=points[:, 1],\n",
    "        z=points[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color=['rgb({},{},{})'.format(r, g, b) for r, g, b in colors],\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z',\n",
    "            aspectmode='data'\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=0)\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def process_multiple_images(image_paths):\n",
    "    model = load_model()\n",
    "    images = []\n",
    "    depth_maps = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        result = process_image(model, path)\n",
    "        if result is not None:\n",
    "            image, depth_map = result\n",
    "            images.append(image)\n",
    "            depth_maps.append(depth_map)\n",
    "\n",
    "    combined_depth = combine_depth_maps(depth_maps)\n",
    "    points, colors = create_point_cloud(images, depth_maps)\n",
    "    fig = plot_3d_point_cloud(points, colors)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "image_paths = ['path/to/image1.jpg', 'path/to/image2.jpg', 'path/to/image3.jpg']\n",
    "fig = process_multiple_images(image_paths)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and saving data...\n",
      "Dataset structure:\n",
      "   product_id                     main_image                   other_images\n",
      "0  0615916821  ../data/small/20/208bd166.jpg                               \n",
      "1  B00004SD6V  ../data/small/44/448ff753.jpg  ../data/small/88/88887e1c.jpg\n",
      "2  B00004TBMT  ../data/small/6d/6d27b089.jpg  ../data/small/aa/aa52d129.jpg\n",
      "3  B00004W42A  ../data/small/cf/cf93a1e1.jpg                               \n",
      "4  B000050415  ../data/small/1c/1c4e48da.jpg                               \n",
      "5  B000050419  ../data/small/a9/a96e15d9.jpg  ../data/small/fc/fc97fc19.jpg\n",
      "6  B00005041B  ../data/small/d1/d1aed412.jpg                               \n",
      "7  B00005041G  ../data/small/b8/b8539919.jpg                               \n",
      "8  B00005041J  ../data/small/0f/0f1a37c5.jpg                               \n",
      "9  B00005041K  ../data/small/91/913cba1f.jpg                               \n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52401 entries, 0 to 52400\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   product_id    52401 non-null  object\n",
      " 1   main_image    46071 non-null  object\n",
      " 2   other_images  52401 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def process_and_save_csv(input_csv_path, output_csv_path, max_products=None):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Extract product_id from meta\n",
    "    df['product_id'] = df['meta'].str.split(':').str[0]\n",
    "    \n",
    "    # Create is_main column\n",
    "    df['is_main'] = df['meta'].str.endswith(':main')\n",
    "    \n",
    "    # Add full path to images\n",
    "    df['full_path'] = \"../data/small/\" + df['path']\n",
    "    \n",
    "    # Group by product_id and aggregate\n",
    "    grouped = df.groupby('product_id').agg({\n",
    "        'full_path': lambda x: '|'.join(x),\n",
    "        'is_main': lambda x: '|'.join(x.astype(str))\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Separate main image and other images\n",
    "    def split_images(row):\n",
    "        paths = row['full_path'].split('|')\n",
    "        is_mains = row['is_main'].split('|')\n",
    "        main_image = next((path for path, is_main in zip(paths, is_mains) if is_main == 'True'), None)\n",
    "        other_images = [path for path, is_main in zip(paths, is_mains) if is_main == 'False']\n",
    "        return pd.Series({'main_image': main_image, 'other_images': '|'.join(other_images)})\n",
    "\n",
    "    grouped[['main_image', 'other_images']] = grouped.apply(split_images, axis=1)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    grouped = grouped.drop(columns=['full_path', 'is_main'])\n",
    "    \n",
    "    if max_products:\n",
    "        grouped = grouped.head(max_products)\n",
    "    \n",
    "    # Save to CSV\n",
    "    grouped.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "def load_processed_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['other_images'] = df['other_images'].apply(lambda x: x.split('|') if pd.notna(x) else [])\n",
    "    return df\n",
    "\n",
    "# Main execution\n",
    "input_csv_path = '../data/metadata/abo-mvr.csv'  # Replace with your actual input CSV file path\n",
    "processed_csv_path = './metadata.csv'  # Replace with desired output CSV file path\n",
    "max_products = None  # Set to a number if you want to limit the products processed\n",
    "\n",
    "# Check if processed CSV exists\n",
    "if os.path.exists(processed_csv_path):\n",
    "    print(\"Loading pre-processed data...\")\n",
    "    dataset = load_processed_csv(processed_csv_path)\n",
    "else:\n",
    "    print(\"Processing and saving data...\")\n",
    "    dataset = process_and_save_csv(input_csv_path, processed_csv_path, max_products)\n",
    "\n",
    "# Print information about the dataset\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset.head(10))\n",
    "print(\"\\nDataset info:\")\n",
    "print(dataset.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
